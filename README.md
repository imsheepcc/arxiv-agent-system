# Multi-Agent Code Generation System

A sophisticated multi-agent collaborative system that autonomously generates complete software projects from natural language descriptions. Built for COMP7103C Course Assignment.

## ğŸ¯ Project Overview

This system implements a multi-agent architecture where specialized AI agents collaborate to:
- **Plan** software projects and decompose requirements into tasks
- **Generate** production-ready code with proper structure
- **Evaluate** code quality and completeness

## ğŸ—ï¸ Architecture

### Core Components

#### 1. **Agents**
- **Planning Agent**: Analyzes requirements, designs architecture, creates task lists
- **Code Generation Agent**: Implements features using file system tools and LLM function calling
- **Evaluation Agent**: Reviews code quality, validates functionality, provides feedback

#### 2. **Orchestrator**
- **Task Scheduling**: Manages task execution order and dependencies
- **Communication Management**: Coordinates information flow between agents
- **State Management**: Tracks project state and completion status

#### 3. **Tools**
- **File System Tools**: Create, read, write, and manage files
- **LLM Client**: Unified interface for multiple LLM providers (DeepSeek, OpenAI, Claude)
- **arXiv API Tools**: Fetch REAL papers from arXiv (search by category, get authentic metadata)
- **Web Search Tools**: Search the web (Brave, Google Serper, or DuckDuckGo)

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone <your-repo-url>
cd arxiv-agent-system

# Install dependencies
pip install -r requirements.txt
```

### Configuration

Set up your LLM API key (choose one):

```bash
# For DeepSeek (recommended)
export DEEPSEEK_API_KEY="your-api-key-here"

# For OpenAI
export OPENAI_API_KEY="your-api-key-here"

# For Anthropic Claude
export ANTHROPIC_API_KEY="your-api-key-here"
```

### Basic Usage

```bash
# Run the arXiv CS Daily project (default)
python main.py

# Specify custom output directory
python main.py --output-dir my_project

# Use different LLM provider
python main.py --provider openai

# Test without API (mock mode)
python main.py --mock
```

### Advanced Usage

```bash
# Custom project requirement
python main.py --requirement "Build a todo list web app with HTML, CSS, and JavaScript"

# Specify model and provider
python main.py --provider deepseek --model deepseek-chat

# Control iteration limit
python main.py --max-iterations 30
```

## ğŸ“ Project Structure

```
arxiv-agent-system/
â”œâ”€â”€ agents/                 # Agent implementations
â”‚   â”œâ”€â”€ base_agent.py      # Base agent class and PlanningAgent
â”‚   â”œâ”€â”€ code_agent.py      # Code generation agent
â”‚   â””â”€â”€ evaluation_agent.py # Code evaluation agent
â”œâ”€â”€ orchestrator/          # Multi-agent coordination
â”‚   â””â”€â”€ multi_agent_orchestrator.py
â”œâ”€â”€ tools/                 # Agent tools
â”‚   â”œâ”€â”€ file_tools.py     # File system operations
â”‚   â””â”€â”€ llm_client.py     # LLM API wrapper
â”œâ”€â”€ prompts/              # System prompts
â”‚   â””â”€â”€ system_prompts.py
â”œâ”€â”€ config/               # Configuration
â”‚   â””â”€â”€ config.py
â”œâ”€â”€ outputs/              # Generated code (created at runtime)
â”œâ”€â”€ logs/                 # Execution logs (created at runtime)
â”œâ”€â”€ main.py              # Main entry point
â”œâ”€â”€ requirements.txt     # Python dependencies
â””â”€â”€ README.md           # This file
```

## ğŸ“ Test Case: arXiv CS Daily Website

The default test case generates a complete "arXiv CS Daily" website with:

### Features
1. **Domain-Specific Navigation**: Browse by CS categories (cs.AI, cs.CV, cs.LG, etc.)
2. **Daily Paper List**: View latest papers with titles, dates, and categories
3. **Paper Detail Pages**: Full metadata, PDF links, and citation generation (BibTeX + standard format)

### Generated Files
- `index.html` - Homepage with category navigation
- `category.html` - Category-specific paper listings
- `paper.html` - Individual paper details
- `css/style.css` - Responsive styling
- `js/script.js` - Dynamic functionality and citation copying
- `data/papers.json` - Sample paper data

### Running the Website

After generation:
```bash
cd outputs
python -m http.server 8000
# Visit http://localhost:8000 in your browser
```

## ğŸ”§ System Design

### Agent Communication Flow

```
User Requirement
      â†“
[Planning Agent]
      â†“ (Task List)
[Orchestrator] â† â†’ [Code Generation Agent] â† â†’ [File Tools]
      â†“
[Evaluation Agent]
      â†“
Complete Project
```

### Agent Communication Protocol

Agentså’Œè°ƒåº¦å™¨ä¹‹é—´çš„æ¶ˆæ¯ç»Ÿä¸€éµå¾ª `protocol/message_schema.py` ä¸­å®šä¹‰çš„ `AgentMessage`ï¼š

| å­—æ®µ        | å«ä¹‰                                      |
|-------------|-------------------------------------------|
| `id`        | å…¨å±€å”¯ä¸€æ¶ˆæ¯ID (UUID)                      |
| `msg_type`  | æ¶ˆæ¯ç±»å‹ï¼Œå–è‡ª `MessageType` æšä¸¾          |
| `sender`    | å‘é€æ–¹è§’è‰²ï¼ˆå¦‚ Orchestratorã€PlanningAgentï¼‰ |
| `receiver`  | æ¥æ”¶æ–¹è§’è‰²                                  |
| `payload`   | ä¸è¯¥æ¶ˆæ¯ç›¸å…³çš„ä¸šåŠ¡æ•°æ® (ä»»åŠ¡ã€ç»“æœç­‰)      |
| `timestamp` | ISO8601 UTC æ—¶é—´æˆ³                         |

å½“å‰æ”¯æŒçš„ `msg_type`ï¼š

- `plan_request` / `plan_response`
- `task_assignment` / `task_result`
- `evaluation_request` / `evaluation_report`

Orchestrator åœ¨å„é˜¶æ®µéƒ½ä¼šå°è£…åè®®æ¶ˆæ¯å¹¶è®°å½•åˆ°å¯¹åº” Agent çš„ `conversation_history`ï¼Œä¾‹å¦‚ä»»åŠ¡æ´¾å‘ï¼š

```json
{
  "id": "4d9ceff0-77be-4b10-8046-2f5d7fa7c7c0",
  "msg_type": "task_assignment",
  "sender": "Orchestrator",
  "receiver": "CodeGenerationAgent",
  "payload": {
    "task": {
      "id": 3,
      "title": "Create paper detail page",
      "description": "...",
      "file_path": "paper.html"
    },
    "context": {
      "completed_tasks": [1, 2],
      "completed_files": ["data/papers.json", "index.html"]
    }
  },
  "timestamp": "2025-01-15T10:02:30.123Z"
}
```

Evaluation é˜¶æ®µåŒæ ·ä¼šç”Ÿæˆ `evaluation_request` å’Œ `evaluation_report`ï¼Œè¿™æ ·æ—¥å¿—ä¸è¿½è¸ªéƒ½èƒ½åŸºäºç»Ÿä¸€åè®®æ ¼å¼å®Œæˆã€‚

### Key Features

#### 1. Function Calling
Agents use LLM function calling to:
- Create and modify files programmatically
- Read project state
- Execute tools with structured parameters

#### 2. Shared Memory
- **Project Plan**: Accessible to all agents
- **Completed Files**: Track progress across tasks
- **Conversation History**: Maintain context per agent
- **State Manager**: Persisted JSON (`outputs/state/state.json`) keeps project status & agent memories

#### 3. Task Dependency Management
- Automatic dependency resolution
- Priority-based task scheduling
- Iterative refinement capability

### State & Memory Management

ç³»ç»Ÿé€šè¿‡ `state/state_manager.py` å°†è¿è¡ŒçŠ¶æ€æŒä¹…åŒ–è‡³ `outputs/state/state.json`ï¼š

- `project_plan` / `completed_tasks` / `created_files` / `task_results` / `evaluation`
- `agents`: ä¿å­˜æ¯ä¸ª Agent çš„ `conversation_history` ä¸ `thoughts`
- `last_updated`: ISO8601 æ—¶é—´æˆ³

è¿è¡Œè¿‡ç¨‹ä¸­ Orchestrator ä¼šï¼š

1. åŠ è½½å·²æœ‰ stateï¼Œæ¢å¤å„ Agent çš„è®°å¿†
2. æ¯æ¬¡ä»»åŠ¡/è¯„ä¼°å®Œæˆå `state_manager.update(...)`
3. è°ƒç”¨ `state_manager.record_agent_memory(agent)` å†™å›è®°å¿†
4. æä¾› `get_recent_tasks()/get_recent_files()` ä¾› Agent åœ¨ `context` ä¸­å¼•ç”¨ï¼Œå®ç°â€œè®°å¿†å¤ç”¨â€

ç¤ºä¾‹ç‰‡æ®µï¼š

```json
{
  "project_plan": {"project_name": "arXiv CS Daily", "...": "..."},
  "completed_tasks": [1, 2, 3],
  "agents": {
    "CodeGenerationAgent": {
      "conversation_history": [...],
      "thoughts": ["[2025-01-15 10:01:00] Starting task ..."]
    }
  },
  "last_updated": "2025-01-15T10:05:30.456Z"
}
```

## ğŸ“Š Logging and Debugging

The system provides detailed logging:

```python
# Logs are saved to logs/agent_system_TIMESTAMP.log
# Console output shows:
# - Agent thoughts and decisions
# - Tool executions
# - Task progress
# - Evaluation results
```

Example log output:
```
[2024-01-15 10:30:15] [PlanningAgent] Thought: Analyzing requirement...
[2024-01-15 10:30:20] [CodeGenerationAgent] Calling tool: create_file
[2024-01-15 10:30:21] [Orchestrator] âœ“ Task 1 completed - Files: index.html
```

## ğŸ”Œ LLM Provider Support

### Supported Providers
- **DeepSeek** (default): Cost-effective, good performance
- **OpenAI**: GPT-4, GPT-3.5-turbo
- **Anthropic**: Claude 3.5 Sonnet

### Adding New Providers

Edit `tools/llm_client.py`:
```python
self.model_map = {
    "your_provider": "model-name"
}

self.base_url_map = {
    "your_provider": "https://api.provider.com"
}
```

## ğŸ§ª Testing

### Mock Mode (No API Required)
```bash
python main.py --mock
```

The mock client returns simulated responses, useful for:
- Testing system architecture
- Debugging agent communication
- Demo without API costs

### With Real API
```bash
# Recommended for production use
export DEEPSEEK_API_KEY="your-key"
python main.py
```

## ğŸ“ˆ Performance Considerations

- **API Costs**: Use `--mock` for testing, DeepSeek for cost-effective production
- **Iteration Limits**: Default 20, adjust with `--max-iterations`
- **Model Selection**: Cheaper models for planning, better models for code generation

### Recommended Configuration

**Development:**
```bash
python main.py --mock
```

**Production:**
```bash
python main.py --provider deepseek --max-iterations 15
```

## ğŸ› ï¸ Extending the System

### Adding New Agents

```python
# agents/custom_agent.py
from agents.base_agent import BaseAgent

class CustomAgent(BaseAgent):
    def execute(self, task, context=None):
        # Implement custom logic
        pass
```

### Adding New Tools

```python
# tools/custom_tools.py
def my_custom_tool(param1, param2):
    return {"status": "success", "result": ...}
```

### Custom Requirements

```python
# Create your own requirement
custom_req = """
Build a portfolio website with:
- Home page
- Projects gallery
- Contact form
"""

python main.py --requirement "$custom_req"
```

## ğŸ“ Assignment Deliverables

This project provides:

âœ… **Git Repository**: Complete source code with modular architecture  
âœ… **README.md**: Comprehensive documentation (this file)  
âœ… **Functional System**: Generates complete arXiv CS Daily website  
âœ… **Logging**: Detailed execution logs for analysis  
âœ… **Extensibility**: Easy to add agents, tools, and features  

## ğŸ¯ Learning Outcomes Demonstrated

- âœ… Multi-agent architecture design with specialized roles
- âœ… LLM API integration (DeepSeek, OpenAI, Claude)
- âœ… Function calling implementation for agent tools
- âœ… Advanced agent communication protocols
- âœ… Task decomposition and collaborative execution
- âœ… Code quality evaluation and feedback systems

## ğŸ› Troubleshooting

### Common Issues

**"API Error"**
- Check API key is set correctly
- Verify internet connection
- Try `--mock` mode for testing

**"No files created"**
- Check logs for errors
- Verify output directory permissions
- Try with `--max-iterations 30`

**"Planning failed"**
- LLM may have returned invalid JSON
- System uses fallback plan automatically
- Check logs for details

## ğŸ“š References

- LangChain: Agent framework inspiration
- AutoGen: Multi-agent patterns
- OpenAI Function Calling: Tool use patterns

## ğŸ‘¥ Authors

Kasper - COMP7103C Course Assignment

## ğŸ“„ License

MIT License - For educational purposes

---

## ğŸ’¡ Example Execution

```bash
$ python main.py

[2024-01-15 10:00:00] [Orchestrator] INFO: STARTING MULTI-AGENT SOFTWARE DEVELOPMENT
[2024-01-15 10:00:01] [PlanningAgent] INFO: Analyzing requirement...
[2024-01-15 10:00:05] [PlanningAgent] INFO: Plan created successfully: arXiv CS Daily
[2024-01-15 10:00:06] [CodeGenerationAgent] INFO: Generating code for: index.html
[2024-01-15 10:00:12] [Orchestrator] INFO: âœ“ Task 1 completed - Files: index.html
...
[2024-01-15 10:05:00] [EvaluationAgent] INFO: Evaluation Score: 85/100
[2024-01-15 10:05:00] [Orchestrator] INFO: PASSED

âœ“ Project completed successfully!
âœ“ Generated 6 files
âœ“ Check output at: /path/to/outputs
```

---

For questions or issues, please check the logs or refer to the course TAs.
