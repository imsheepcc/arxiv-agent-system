1 项目概况
本项目为 COMP7103C 课程作业，项目名称为 Multi-Agent Code Generation System，选取的典型用例是“arXiv CS Daily”网站生成任务。系统运行环境为 Windows 平台下的 Python 项目，核心目标是将自然语言需求作为输入，通过多智能体协作自动产出一套可直接运行、结构完整的软件项目。围绕课程要求，本系统不仅关注“能生成代码”，还强调生成流程的可追踪性、可复现性以及对不同大模型服务的适配能力。

1.1 项目目标
从目标上看，系统以自然语言需求为驱动；在默认场景下，需求来自 `prompts/system_prompts.py` 中定义的 `ARXIV_PROJECT_REQUIREMENT`，描述了需要构建一个具备学科分类导航、论文列表浏览、论文详情页展示以及引用生成等功能的 arXiv 计算机科学每日更新网站。系统的输出是一个可运行的前端静态网站工程，生成结果被写入 `outputs/` 目录，包含 HTML、CSS、JavaScript 以及承载论文元数据的 JSON 文件等，使得用户无需手工搭建工程结构即可获得可部署的网页产物。

1.2 方法概述
在方法上，本系统采用多代理（multi-agent）协作范式，将“规划—实现—评估”拆解为三个职责清晰的角色：PlanningAgent 负责理解需求并形成结构化计划与任务列表；CodeGenerationAgent 按任务逐步生成对应的文件内容，并通过工具调用落地到文件系统；EvaluationAgent 在生成完成后对代码质量、功能完备性与用户体验进行整体评估并给出改进建议。三类代理由 MultiAgentOrchestrator 统一编排，按照阶段化流程驱动执行，并在任务执行过程中维护共享上下文与依赖关系，从而降低复杂需求下的失配风险。系统同时提供工具层支撑，其中 FileTools 负责文件创建与读写，LLMClient 则封装不同大模型提供商的调用差异，并支持在缺少 API Key 或需要演示时降级到 mock 模式，以提升可用性与可移植性。

1.3 结果概述
从结果来看，系统最终能够生成包含首页、分类页与论文详情页在内的完整网站结构，并通过 JavaScript 读取 `data/papers.json` 完成数据渲染、过滤与引用文本生成等关键交互功能。根据运行过程中持久化的评估信息（见 `outputs/state/state.json`），本次生成结果的整体评分为 88/100，评估结论为通过；评审同时指出了可进一步完善之处，例如在数据拉取失败时提供更健壮的错误处理与更友好的用户提示。整体而言，该系统验证了多代理协作在“从需求到工程产物”自动化流程中的可行性，并为后续扩展真实 arXiv 数据抓取、增强自动化测试与提升生成稳定性提供了明确方向。

2 引言
首先说明本项目的动机：在大模型逐步具备“理解需求、生成方案、产出代码”的能力之后，如何将这种能力以工程化方式组织起来，形成稳定、可追踪、可复现的自动化开发流程，成为软件工程与数据挖掘课程实践中的一个关键问题。相比单一模型一次性产出整套代码，多代理分工能够把复杂需求拆解为可管理的子问题，并通过清晰的职责边界提升可控性与可扩展性。

2.1 需求与任务定义
在需求与任务定义方面，系统默认以“arXiv CS Daily”作为测试需求，目标是生成一个具备学科分类导航、论文列表浏览、论文详情页展示与引用生成等功能的静态网站。项目通过 `main.py` 提供统一入口，支持用 `--provider/--model` 选择不同 LLM 提供商与模型，也可通过 `--mock` 在无 API Key 场景下演示运行，并用 `--output-dir` 与 `--max-iterations` 控制生成产物位置与迭代次数。这些参数化设计使得系统既能在真实 API 环境下运行，也能在课堂展示与调试阶段保持可用。

2.2 贡献点
本项目的主要贡献体现在三个方面：其一是实现了可复用的多代理编排框架，由 `MultiAgentOrchestrator` 负责分阶段调度与依赖管理；其二是定义了统一的代理通信协议（`protocol/message_schema.py`），便于记录与回放关键交互；其三是通过 `StateManager` 将计划、执行进度与代理记忆持久化，从而支持状态恢复与跨阶段上下文共享，为后续扩展与实验对比提供基础。

3 System Design（系统设计）
本章从系统实现出发，对 Multi-Agent Code Generation System 的整体结构与关键数据流进行说明。系统的核心思想是以“编排器 + 多角色代理 + 工具层”的方式把自然语言需求转化为可执行任务，再由工具调用将生成结果写入到实际工程目录中，并最终形成可运行的项目产物。为了保证过程可追踪与可恢复，系统在运行期间会将计划、任务状态、生成文件与评估结论持久化到输出目录中的状态文件中，从而支持复盘与后续迭代。

3.1 总体架构与分层
从工程结构上看，系统可以概括为五个相互协作的层次。入口层负责启动与参数解析，并完成必要的初始化；为保证可复现实验与便于切换运行环境，系统将关键选择暴露为命令行参数，例如通过 `--provider` 与 `--model` 选择模型提供商与模型版本，通过 `--mock` 在无密钥或离线条件下演示流程，并用 `--output-dir` 统一指定生成产物落地位置。编排层将一次运行组织为“规划—生成—评估”的阶段化流程，并在执行中负责依赖检查、任务排序与结果汇总，从而把复杂需求拆成可管理的子目标。代理层分别承担需求拆解、实现落地与质量评审等职责，并在共享上下文的约束下进行增量式协作，以减少重复生成与信息偏差。工具层为代理提供受控的模型调用与文件读写能力，使生成内容能够被稳定写入输出目录并可随时读取校验。状态层则将阶段性进度与关键结论持久化，支持中断恢复与过程复盘，保证系统运行不依赖临时上下文。

3.2 核心执行流程（Phase-based Workflow）
系统的核心执行以阶段化（phase-based）的方式组织，可概括为“规划—生成—评估”三步。首先在规划阶段，编排器将原始需求交给规划代理进行分析，并产出结构化的项目计划：明确目标功能、推荐的文件组织方式以及可依次执行的任务列表；该计划会被记录到状态文件中，作为后续生成阶段的参考基线，避免执行过程中偏离需求。随后在代码生成阶段，编排器按照任务依赖与优先级逐项推进，并将已完成的任务与已有产物作为上下文交给生成代理，使其以增量方式完善工程文件，最终在输出目录中形成可运行的前端页面、静态资源与数据文件。最后在评估阶段，系统汇总关键生成文件与需求对照进行检查，从功能完备性、代码质量与用户体验三个角度给出综合评价，并将评分与改进建议写回状态记录（例如 `outputs/state/state.json`），为后续迭代提供依据。

3.3 Agent 设计（职责与接口）
本系统的代理设计遵循“分工明确、信息对齐、可迭代改进”的规划逻辑：先由负责全局理解的角色把需求拆成可执行单元，再由负责落地实现的角色逐个完成任务，最后由负责质量把关的角色对整体产物做一致性检查并提出改进建议。这样的角色划分能够把复杂问题分解为一系列更易控制的决策点，并将每一步的输入与输出限定在清晰边界内，从而降低一次性生成整套工程时常见的遗漏与失配风险。

具体而言，规划代理的核心价值不在于“写代码”，而在于把自然语言需求转写为结构化计划：明确目标产物是什么、需要哪些文件与模块、各项功能的先后顺序以及依赖关系如何安排。规划阶段形成的任务列表相当于后续执行阶段的路线图，编排器据此进行任务排序与依赖检查，避免出现尚未生成数据文件就开始渲染页面、或缺少公共样式却先完成页面细节等不合理的执行顺序。

代码生成代理承担“按任务推进”的执行职责，其重点是把每个任务视为一个小型交付：给定目标文件与上下文约束，优先通过工具把内容真实写入工程目录，并在多轮交互中逐步收敛到可运行的实现。与一次性输出大量代码相比，这种面向任务的执行方式更便于控制范围、减少返工，并允许编排器在每个任务完成后将新的产物与状态反馈给后续任务，从而形成增量式的工程构建过程。

评估代理则负责把执行结果重新与原始需求对齐，重点审查“做出来的东西是否可用、是否完整、是否符合用户体验预期”，并将问题转化为可操作的改进建议，作为下一轮迭代的输入。通过将评估从生成环节中剥离出来，系统可以在不打断执行节奏的前提下获得更客观的质量反馈，同时也便于在未来引入自动化测试、静态检查等更强的验证手段。

3.4 通信协议（Message Schema & Traceability）
为了让多代理协作具备可追踪性与可审计性，系统采用统一的通信协议来描述“谁在何时向谁发送了什么信息”。在执行过程中，编排器会把规划请求、任务派发、任务完成回执以及评估请求与评估报告等关键交互都编码成结构化消息，并将这些消息记录到代理的对话历史中。这样做的意义在于，即使不同代理在不同阶段关注点不同，系统也能以一致的数据结构对交互过程进行归档，从而支持复盘与错误定位。

从流程角度看，协议消息相当于协作的“流水账”：规划阶段通过请求与响应把需求与计划绑定起来；执行阶段通过任务分配与结果回传把每一次产出与对应任务关联起来；评估阶段通过评估请求与报告把最终结论与改进建议固化下来。由于消息记录与阶段边界一致，系统在出现异常（例如规划输出不完整、某任务生成失败、或评估发现关键缺陷）时，可以快速定位问题发生在协作链路的哪个环节，并据此采取重试、回退或重新规划等策略。

3.5 状态与记忆（State & Memory）
多代理系统的执行往往跨越多个步骤与多次模型调用，如果没有持久化机制，任何中断或错误都可能导致上下文丢失、重复生成以及难以复现的问题。为此，本系统将项目计划、执行进度、阶段性产物与评估结论写入状态文件，并把各代理在执行过程中累积的对话历史与思考记录一并保存，从而把“运行时上下文”转化为可长期保存的“过程证据”。

在执行层面，状态持久化直接服务于编排逻辑：编排器能够从状态中获取已完成任务与已生成文件，进而在派发新任务时提供更准确的上下文，避免代理重复创建同一文件或忽略已有实现；当需要恢复执行时，系统也可以从上次保存的状态继续推进，而不是重新从零开始。与此同时，系统还通过维护“最近生成内容”的短期记忆来增强上下文对齐，使后续任务能够基于最新产物做增量调整，提升整体工程的一致性与连贯性。

3.6 工具层与外部依赖
系统之所以能够把“模型输出的文本”转化为“真实可运行的工程”，关键在于工具层为代理提供了受控的执行能力，并将外部依赖与工程操作抽象成稳定接口。对代理而言，工具层相当于一个可调用的“行动空间”：当规划阶段确定了需要的文件与功能后，生成阶段不需要一次性写出所有内容，而是可以通过多轮交互逐步调用工具完成文件创建、内容写入与必要的上下文读取，从而让执行过程更可控、也更便于定位失败点。

在语言模型接入方面，系统采用统一的 LLM 客户端封装不同模型提供商的差异，使得上层编排逻辑可以用同一套消息与工具定义驱动不同后端。当切换 provider 或 model 时，变化主要发生在请求的发送方式与鉴权配置，而不会影响“规划—生成—评估”三阶段的整体流程。与此同时，系统也提供了无 API Key 场景下的降级策略，通过 mock 模式让流程仍可演示和调试；这对于项目环境下的可复现性尤其重要，因为它把“外部服务可用性”与“系统架构正确性”解耦开来。

在工程操作方面，文件工具将写文件、读文件、列目录等动作限制在输出目录内，一方面降低误操作风险，另一方面也让生成结果具备清晰边界：所有产物都集中在 outputs 目录，便于统一打包、展示与测试。配置文件则承担运行参数的集中管理，使用户能够通过少量配置项控制默认模型、备选模型与编排器参数等，从而在不同运行场景中快速切换成本与效果。

3.7 生成产物结构（Generated Artifacts）
系统执行完成后，会在输出目录中生成一套结构清晰的静态网站工程，其组织方式与规划阶段的文件结构保持一致，从而体现“计划驱动实现”的执行特点。典型产物包含首页、分类页与论文详情页等页面文件，以及用于统一样式和交互逻辑的静态资源目录；此外还包含承载论文元数据的 JSON 数据文件，以及用于说明如何本地运行与查看的使用文档和状态文件。通过这种组织方式，产物既能直接用简单 HTTP Server 运行展示，也能作为后续扩展（例如增加搜索、分页、真实数据抓取）的基础工程。

从运行时的数据流角度看，前端页面并不是把数据写死在 HTML 中，而是通过脚本在浏览器端加载 JSON 数据，再将其渲染为分类导航、论文列表与详情页内容。这样做的好处是数据与展示逻辑解耦：当数据源更新或扩展时，只需要替换或更新数据文件即可驱动页面展示变化，而不必重写页面结构。同时，引用生成与复制等交互功能也可以在统一的脚本逻辑中复用，从而保证不同页面之间的体验一致性。

4 Key Challenges and Solutions（关键挑战与解决方案）
本章总结系统在“从自然语言到可运行工程”的端到端自动化过程中遇到的主要挑战，并说明当前实现中采用的应对策略与可能的改进方向。由于系统以多代理协作与工具调用为核心机制，挑战往往集中在三个方面：结构化表达是否可靠、跨代理协作是否一致、以及外部依赖与运行环境是否会影响可复现性。

4.1 需求到可执行任务的结构化表达
多代理系统的第一道关卡在于把自然语言需求转换为编排器可执行的结构化计划。真实运行中，模型输出可能包含解释性文字、格式偏差或不完整的 JSON 片段，导致计划无法被稳定解析，进而影响后续任务派发。为降低该风险，规划阶段采用“尽量结构化 + 失败可降级”的策略：优先从模型输出中抽取计划 JSON（例如通过定位最外层花括号范围来截取），一旦解析失败就触发 fallback plan，保证系统仍能产出最小可运行的文件集合并完成基本流程。通过这种方式，系统将不确定性前置在规划阶段消化，避免把解析失败扩散到整个执行链路。

4.2 多代理协作的一致性与上下文对齐
在多代理分工场景下，一致性问题通常表现为信息丢失、任务重复执行、或因依赖关系处理不当导致生成顺序错乱。为此，系统将“统一调度”放在编排器层完成：编排器根据任务的依赖关系与优先级组织执行顺序，并在每次派发任务时注入必要上下文，例如已完成任务与已生成文件列表（如 `completed_tasks`、`created_files`），以及近期产物摘要（recent files/tasks），让执行代理能够基于最新状态做增量生成。同时，系统通过状态持久化把阶段性结果固化下来，形成“单一事实来源”，减少不同代理对工程状态产生不一致理解的概率。

4.3 工具调用的可靠性（Function Calling Robustness）
工具调用机制提升了“落地能力”，但也引入了新的不确定性：模型可能重复触发相同工具调用，或在多轮交互中产生冗余写入，最终表现为状态记录重复（例如 `created_files` 或任务结果中的 `files_created` 出现重复项）。当前实现的基本策略是用结构化工具返回结果推进对话，使编排器与代理能够在每轮工具执行后得到可校验的反馈，并据此继续下一步操作。面向更稳定的工程化落地，后续可在编排器侧增加幂等性与去重机制：例如在写入状态前对文件列表做集合化处理，或在执行工具前基于目标路径与写入模式进行重复调用检测，从而把“重复写入”从模型行为问题转化为系统层的可控约束。

4.4 提供商差异与可移植性（Provider Compatibility）
不同模型提供商在 API 协议、消息格式与工具调用表示上存在差异，这会直接影响多代理系统的可移植性与长期维护成本。系统采用统一的客户端抽象来屏蔽这些差异：上层始终以一致的消息结构与 `chat` 调用方式驱动模型，而底层根据 provider 选择不同的请求/响应适配路径，并在需要时进行工具格式转换（例如将通用工具定义转换为特定提供商可接受的 schema）。这种封装使得“切换 provider/model”更多是运行参数层面的选择，而不是架构层面的重写，从而提高了系统在不同环境下复用与扩展的能力。

4.5 可复现与可调试性
自动化生成系统常见的痛点是难以解释“为什么这样生成”，以及一旦结果不符合预期难以定位原因。为应对这一问题，系统强调过程留痕：一方面通过统一日志将关键阶段、任务派发、工具执行与评估结论记录到日志文件（例如 `logs/agent_system_*.log`），另一方面让代理在关键节点记录思考摘要（如 `thoughts`），并配合协议化消息把规划请求、任务分配与评估报告等交互以结构化形式存档。这样一来，调试不再只能依赖最终产物对比，而可以沿着“计划—任务—工具调用—产物—评估”的链路逐步回放与定位问题。

4.6 数据真实性与外部接口依赖（arXiv API）
在以真实数据为目标的数据驱动项目中，外部接口依赖是不可回避的挑战：需求强调必须调用 arXiv API 获取真实论文，但在离线或无 API Key 的场景下，系统无法保证数据真实性与实时性。对此，系统采用“能力可选 + 降级可运行”的折中策略：在外部工具可用时启用真实数据获取流程，在不可用时允许使用样例数据保障网站仍可运行与展示，同时在报告中明确该限制并将其归为未来改进方向。进一步的工程化方案是引入专门的数据抓取脚本与缓存机制，把在线接口调用从运行时关键路径中剥离出来，使得生成流程在不稳定网络条件下依然具备可控性与可复现性。

5 Results Analysis（结果分析）
本部分从“需求完成度、产物结构质量、数据覆盖情况、评估结果以及测试验证”五个维度对系统输出进行分析。与系统设计章节的过程性描述不同，这里更强调生成结果本身是否可用、是否满足需求意图，以及当前实现与原始要求之间仍存在的差距。

5.1 功能完成度（按需求对照）
从页面功能角度看，生成的站点已经具备较完整的浏览与展示链路。首页侧重于分类导航与入口组织，用户可以通过页面上的分类入口进入对应的论文列表；分类页支持按领域过滤，常见实现方式是读取 URL 查询参数（例如 `category`）并据此筛选数据源中的论文记录，从而实现 cs.AI、cs.CV 等不同主题之间的快速切换。详情页则聚焦于单篇论文的核心信息展示，并提供外链跳转（如 PDF 或 arXiv 页面）以及引用生成与复制等交互能力，形成“浏览—筛选—查看详情—引用”一条较为完整的使用路径。

进一步看列表与详情的内容组织，分类页通常以卡片形式呈现论文标题、作者、类别与提交日期等关键信息，并辅以摘要片段以提升可读性；详情页在此基础上扩展出更完整的元数据与引用文本输出。引用生成部分是本用例中的亮点功能之一，它将用户最常见的学术写作需求直接内置到浏览流程中，降低了从阅读到引用的操作成本。

5.2 生成产物与结构质量
从产物角度看，系统输出遵循“静态前端 + 数据文件”的组织方式，代码结构清晰、可部署成本低，且便于后续在不引入复杂框架的前提下进行功能扩展。生成目录中页面文件与静态资源目录职责分离，样式与脚本集中管理，使得页面之间可以共享一致的视觉与交互体验。与此同时，生成的使用说明（例如 `outputs/USAGE.md`）为本地启动与访问路径提供了直接指引，降低了结果验收与演示的门槛。整体而言，这种产物结构与规划阶段的“文件结构先行”策略一致，体现了计划对落地结果的约束作用。

5.3 数据规模与覆盖
数据文件是该站点能否“看起来像真的在更新”的关键依赖。从 `outputs/data/papers.json` 的当前内容来看，数据格式已经包含论文 ID、标题、作者、摘要、分类与链接等字段，能够支持前端完成分类筛选、列表渲染与详情定位；同时分类集合也覆盖了 AI、CV、LG、CL、RO、CR 等多个常见方向，满足“多领域浏览”的基本需求。

不过，如果严格对照原始需求中“多类别、25–30 篇、并且来自真实 arXiv API”的要求，现阶段的数据规模与真实性仍可能存在不足：样例数据可以验证页面与交互逻辑，但不足以证明系统在真实接口条件下的稳定抓取能力与数据更新能力。这个差距应当在反思部分被明确说明，并作为后续改进的重点方向。

5.4 评估结果解读
从系统产生的评估结果来看，整体评分达到了较高水平，说明生成产物在可用性与体验上基本达标。评估中指出的问题以低严重度为主，集中在数据加载失败时的用户提示与错误处理等细节，这类问题通常不会阻塞核心功能，但会影响真实使用场景下的鲁棒性与体验一致性。评估同时给出了进一步增强产品化体验的建议，例如加入加载指示、搜索与分页等功能；这些建议与当前产物的静态数据规模相匹配，也与未来扩展到更大规模真实数据时的需求相一致。

5.5 测试与验证
在验证方式上，项目提供了基础测试脚本用于快速检查系统可运行性与关键产物是否生成成功。测试覆盖了核心模块的导入、工具类的基本能力、代理初始化流程以及生成文件的存在性与数据文件的 JSON 合法性等，这些检查有助于在迭代开发过程中尽早发现环境问题或关键文件缺失。配合 mock 模式运行，可以在不依赖外部 API 的情况下完成一次端到端的流程回归；而在接入真实模型与真实数据抓取后，则需要进一步补充针对网络请求、数据解析与前端渲染一致性的测试，以支撑更严格的结果验收。

6. Reflections（反思）
本章围绕系统在本次用例中的实际表现，总结经验与收获，明确现阶段的局限性，并提出可执行的改进方向。反思的重点不是重复描述实现细节，而是回答三个问题：多代理架构在实践中带来了哪些真实收益、哪些问题在当前实现下仍难以避免、以及下一步如何以较低成本把系统推进到更可靠、更贴近真实需求的状态。

6.1 经验与收获
首先，多代理分工让复杂需求的执行过程更可控。将“规划—生成—评估”分离后，规划阶段可以把目标文件、依赖关系与交付顺序明确下来，生成阶段则能够在受控范围内逐步推进，评估阶段再以相对独立视角检查结果是否对齐需求。这种分阶段机制减少了“一次性生成整套工程”常见的遗漏与结构混乱，也让每一步的失败更容易被定位并回滚。其次，协议化消息与状态持久化显著提升了可复现性与可调试性：一方面，关键交互被记录为结构化事件，另一方面，执行进度与评估结论被写入 `outputs/state/state.json` 形成可追踪证据链，使得调试不再依赖记忆或人工复盘，而是可以基于日志与状态进行定位与对比。

6.2 局限性
当前实现的主要局限性首先来自外部数据与环境依赖。需求强调必须调用 arXiv API 获取真实论文数据，但在离线或无密钥的条件下，系统只能使用样例数据来验证前端与流程正确性，这意味着“数据真实性、规模与每日更新”这一关键指标无法被完全证明。其次，工具调用的幂等性仍有提升空间：在多轮工具交互中，模型可能触发重复写入，进而造成状态记录重复或产物被多次覆盖，虽然不一定破坏最终可运行性，但会影响过程的整洁性与可审计性。最后，评估阶段仍偏主观，当前更多依赖模型生成的质量意见，缺少更强的自动化测试、静态检查与一致性校验来支撑更严格的质量门槛。

6.3 改进方向（Future Work）
面向后续迭代，可以从数据、前端与系统三个层面分步增强。数据侧应优先补齐真实数据抓取与缓存链路，例如落地 `scripts/fetch_papers.py` 将 arXiv API 获取过程固化为可重复执行的脚本，并引入缓存与增量更新策略，保证“Daily”属性在不稳定网络条件下仍可实现。前端侧可在现有页面结构上增加加载与错误提示、搜索与分页等功能，以适配真实数据规模扩大后的浏览需求，同时逐步增强可访问性（如 ARIA 与键盘操作）以提升通用用户体验。系统侧则应加强幂等性与校验机制：对 `created_files` 等过程记录进行去重，对计划与评估输出做更严格的 schema 校验，并逐步将 lint、单元测试或端到端检查纳入自动化流程（例如在可用时通过命令工具运行测试并写回评估），从而把“看起来可用”进一步提升为“可证明可靠”。

7. 结论（Conclusion）
综上所述，本项目验证了多代理协作范式在“从自然语言需求到可运行项目产物”这一端到端任务上的有效性。通过将开发流程拆分为规划、生成与评估三个阶段，系统能够在复杂需求下保持较清晰的执行路径，并以工具调用的方式把模型输出转化为真实的文件产物，使最终结果可直接运行与展示。实践表明，这种分工式架构相较于单次生成更易控制范围，也更便于在出现问题时进行定位与迭代。

同时，统一的通信协议与状态持久化机制为系统提供了可追踪、可复现的过程记录，使得协作过程能够被审计与回放，降低了“黑盒生成”的不确定性。尽管在真实数据获取、工具幂等性与自动化验证等方面仍有改进空间，但整体设计已经为后续扩展奠定了稳定基础，能够支持在更严格的工程要求下逐步演进。

8. 附录（Appendix，可选）
本附录汇总项目中最关键的文件位置与常用运行命令，便于读者快速定位实现细节与复现实验结果。

8.1 关键文件索引
main.py
orchestrator/multi_agent_orchestrator.py
agents/base_agent.py
agents/code_agent.py
agents/evaluation_agent.py
tools/llm_client.py
tools/file_tools.py
protocol/message_schema.py
state/state_manager.py
outputs/*（生成结果）

8.2 运行命令与参数说明
python main.py [--mock] [--provider deepseek|openai|anthropic] [--output-dir ...] [--max-iterations N]
python test_system.py
